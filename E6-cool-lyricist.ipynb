{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finite-geology",
   "metadata": {},
   "source": [
    "# E6 - (연습) 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-taxation",
   "metadata": {},
   "source": [
    "## 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-description",
   "metadata": {},
   "source": [
    "* `Song Lyrics` 데이터 다운로드 및 `Lyrics` 디렉토리 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-marshall",
   "metadata": {},
   "source": [
    "wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip  \n",
    "unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  \n",
    "#lyrics 폴더에 압축풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-sixth",
   "metadata": {},
   "source": [
    "## 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-psychology",
   "metadata": {},
   "source": [
    "* `glob` 모듈 : 파일 읽어오는 작업에 용이  \n",
    "* `glob` 으로 모든 `txt` 파일을 읽어온 후, `raw_corpus` 리스트에 문장 단위로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-auditor",
   "metadata": {},
   "source": [
    "* 필요한 모듈 불러오기 및 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "victorian-housing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-ambassador",
   "metadata": {},
   "source": [
    "## 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-season",
   "metadata": {},
   "source": [
    "* 문장 다듬기 (원하는 문장만 출력하기) : 불필요한 기호나 문장, 또는 공백 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "satisfied-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-weather",
   "metadata": {},
   "source": [
    "* 텍스트 분류 모델에서 많이 보신 것처럼 텍스트 생성 모델에도 단어 사전을 만들게 됩니다. 그렇다면 문장을 일정한 기준으로 쪼개야겠죠? 그 과정을 토큰화(Tokenize) 라고 합니다.  \n",
    "  \n",
    "* 가장 심플한 방법은 띄어쓰기를 기준으로 나누는 방법이고, 우리도 그 방법을 사용할 겁니다. 하지만 약간의 문제가 있을 수 있죠. 몇 가지 문제 케이스를 살펴보죠.  \n",
    "  \n",
    "1. Hi, my name is John. *(\"Hi,\" \"my\", …, \"john.\" 으로 분리됨) - 문장부호  \n",
    "  \n",
    "2. First, open the first chapter. *(First와 first를 다른 단어로 인식) - 대소문자  \n",
    "   \n",
    "3. He is a ten-year-old boy. *(ten-year-old를 한 단어로 인식) - 특수문자  \n",
    "  \n",
    "* \"1.\" 을 막기 위해 문장 부호 양쪽에 공백을 추가 할 거고요, \"2.\" 를 막기 위해 모든 문자들을 소문자로 변환할 겁니다. \"3.\"을 막기 위해 특수문자들은 모두 제거하도록 하죠! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-multiple",
   "metadata": {},
   "source": [
    "* 이런 전처리를 위해 정규표현식(Regex)을 이용한 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "political-sheffield",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-cosmetic",
   "metadata": {},
   "source": [
    "* 우리가 구축해야 할 데이터셋 모양  \n",
    "  \n",
    "언어 모델의 입력 문장 :  <start> 나는 밥을 먹었다 = 소스 문장(Source Sentence),  \n",
    "언어 모델의 출력 문장 : 나는 밥을 먹었다 <end> = 타겟 문장(Target Sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-catch",
   "metadata": {},
   "source": [
    "* 위에서 만든 정제 함수를 통해 만든 데이터셋에서 토큰화를 진행한 후 끝 단어 <end>를 없애면 소스 문장, 첫 단어 <start>를 없애면 타겟 문장  \n",
    "* 이 정제 함수를 활용해서 아래와 같이 정제 데이터를 구축합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedicated-gothic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-welsh",
   "metadata": {},
   "source": [
    "* 데이터 준비 끝!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-james",
   "metadata": {},
   "source": [
    "## 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-peninsula",
   "metadata": {},
   "source": [
    "### (1) 데이터 토큰화 및 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-reporter",
   "metadata": {},
   "source": [
    "* `tf.keras.preprocessing.text.Tokenizer` 패키지는 정제된 데이터를 토큰화  \n",
    "* 단어 사전(vocabulary 또는 dictionary라고 칭함)을 만들어주며  \n",
    "* 데이터를 숫자로 변환  \n",
    "* 이 과정을 벡터화(vectorize) 라 하며, 숫자로 변환된 데이터를 텐서(tensor) 라고 칭합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "competent-potter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2639 ...    0    0    0]\n",
      " [   2   36    7 ...    0    0    0]\n",
      " ...\n",
      " [   2   36    7 ...    0    0    0]\n",
      " [   2   13  440 ...    0    0    0]\n",
      " [   2   26   17 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f18641685d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "developmental-firewall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5   91  297   65   57    9  969 6042]\n",
      " [   2   17 2639  873    4    8   11 6043    6  329]\n",
      " [   2   36    7   37   15  164  282   28  299    4]]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력해 보기\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-narrow",
   "metadata": {},
   "source": [
    "* 텐서 데이터는 모두 정수로 구성  \n",
    "* 이 숫자는 다름 아니라, tokenizer에 구축된 단어 사전의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "extraordinary-player",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "27592\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전이 어떻게 구축되었는지 아래와 같이 확인해 보기\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break\n",
    "print(len(tokenizer.index_word))\n",
    "\n",
    "# 2번 인덱스가 `<start>` <- 모든 행이 2로 시작하는 이유 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-petroleum",
   "metadata": {},
   "source": [
    "* 생성된 텐서를 소스와 타겟으로 분리하여 모델이 학습할 수 있게 하기  \n",
    "* 이 과정에서도 텐서플로우가 제공하는 모듈을 사용  \n",
    "* 텐서 출력부에서 행 뒤쪽에 0이 많이 나온 부분 : 정해진 입력 시퀀스 길이보다 문장이 짧은 경우 0으로 패딩(padding)을 채워넣은 것  \n",
    "* 사전에는 없지만 0은 바로 패딩 문자 `<pad>`가 될 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "trying-beaver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "[  50    5   91  297   65   57    9  969 6042    3    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "# 마지막 토큰은 `<end>`가 아니라 `<pad>`일 가능성 높음\n",
    "\n",
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    # tensor에서 `<start>` 잘라내서 타겟 문장 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-america",
   "metadata": {},
   "source": [
    "* `corpus` 내의 첫번째 문장에 대해 생성된 소스와 타겟 문장을 확인해 보았음  \n",
    "* 예상대로 소스는 2(`<start>`)에서 시작, 3(`<end>`)으로 끝난 후, 0(`<pad>`)로 채워져 있음  \n",
    "* 하지만 타겟은 2로 시작하지 않고 소스를 왼쪽으로 한칸 시프트한 형태를 띰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-outline",
   "metadata": {},
   "source": [
    "### (2) 데이터셋 객체 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-default",
   "metadata": {},
   "source": [
    "* 그동안 `model.fit(x_train, y_train, …)` 형태로 `Numpy Array` 데이터셋을 생성하여 model에 제공하는 형태의 학습을 많이 진행  \n",
    "* 텐서플로우를 활용할 경우 텐서로 생성된 데이터를 이용해 `tf.data.Dataset`객체를 생성하는 방법을 흔히 사용  \n",
    "* `tf.data.Dataset`객체는 텐서플로우에서 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의기능을 제공하므로 꼭 사용법 알아 두기  \n",
    "* 이미 데이터셋을 텐서 형태로 생성해 두었으므로, `tf.data.Dataset.from_tensor_slices()` 메소드를 이용해 `tf.data.Dataset`객체를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "equipped-detail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((4, 346), (4, 346)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 4  # 배치사이즈 256일때, 메모리 어쩌고 에러가 떠서 그냥 확 줄여버림. 다른 해결법이 있었지만 아직 이해 불가!!\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    \n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-purpose",
   "metadata": {},
   "source": [
    "### (3) 정리 요약 - \"데이터 전처리\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-samba",
   "metadata": {},
   "source": [
    "* 데이터셋을 생성하기 위한 과정  \n",
    "  \n",
    "1. 정규표현식을 이용한 corpus 생성  \n",
    "2. `tf.keras.preprocessing.text.Tokenizer`를 이용해 `corpus`를 텐서로 변환  \n",
    "3. `tf.data.Dataset.from_tensor_slices()`를 이용해 `corpus` 텐서를 `tf.data.Dataset`객체로 변환  \n",
    "  \n",
    "* dataset을 얻음으로써 데이터 다듬기 과정은 끝  \n",
    "* `tf.data.Dataset`에서 제공하는 `shuffle(), batch()` 등 다양한 데이터셋 관련 기능 이용  \n",
    "  \n",
    "* 이 모든 일련의 과정을 텐서플로우에서의 \"데이터 전처리\"라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-reporter",
   "metadata": {},
   "source": [
    "### (4) 훈련 데이터와 평가 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "important-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175749, 346)\n"
     ]
    }
   ],
   "source": [
    "print(src_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "every-emphasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train 개수:  140599 , enc_val 개수:  35150\n",
      "Source Train: (140599, 346)\n",
      "Target Train: (140599, 346)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 준비, 데이터 분리 관련 모듈들\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 분리, 테스트데이터 사이즈 조절 및 랜덤성 결정\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 0)\n",
    "print('enc_train 개수: ', len(enc_train), ', enc_val 개수: ', len(enc_val))\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-feeling",
   "metadata": {},
   "source": [
    "## 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-tradition",
   "metadata": {},
   "source": [
    "### (1) 모델 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-atlantic",
   "metadata": {},
   "source": [
    "* 만들 모델은 tf.keras.Model을 Subclassing하는 방식으로 만들 것  \n",
    "* 만들 모델에는 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "expired-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 16\n",
    "hidden_size = 64\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-marble",
   "metadata": {},
   "source": [
    "* 입력 텐서에는 단어 사전의 인덱스가 들어 있음    \n",
    "* Embedding 레이어는 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔 줌    \n",
    "* 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-walter",
   "metadata": {},
   "source": [
    "* model 아직 제대로 build되지 않았음  \n",
    "* `model.compile()`을 호출한 적도 없고, 아직 model의 입력 텐서가 무엇인지 제대로 지정해 주지도 않았기 때문  \n",
    "* 이런 경우 아래와 같이 model에 데이터를 아주 조금 태워 보자!!  \n",
    "* model의 input shape가 결정되면서 `model.build()`가 자동으로 호출됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "representative-domain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 346, 7001), dtype=float32, numpy=\n",
       "array([[[-1.01898862e-04, -2.46928375e-05,  1.25283666e-04, ...,\n",
       "          6.28904236e-05,  1.14771747e-05,  3.32719683e-05],\n",
       "        [-2.48694996e-04, -4.37231902e-05,  1.45203390e-04, ...,\n",
       "          9.58687015e-05,  4.36506707e-06, -6.01506053e-06],\n",
       "        [-2.00143942e-04, -6.15712270e-05,  9.43804480e-05, ...,\n",
       "          9.34829950e-05, -1.35117016e-05, -1.10947312e-05],\n",
       "        ...,\n",
       "        [-1.39643077e-03,  1.30454777e-03,  1.06719614e-03, ...,\n",
       "          2.07928111e-04, -6.25650806e-04, -4.54423745e-04],\n",
       "        [-1.39643077e-03,  1.30454777e-03,  1.06719614e-03, ...,\n",
       "          2.07928111e-04, -6.25650806e-04, -4.54423745e-04],\n",
       "        [-1.39643077e-03,  1.30454777e-03,  1.06719614e-03, ...,\n",
       "          2.07928111e-04, -6.25650806e-04, -4.54423745e-04]],\n",
       "\n",
       "       [[-1.01898862e-04, -2.46928375e-05,  1.25283666e-04, ...,\n",
       "          6.28904236e-05,  1.14771747e-05,  3.32719683e-05],\n",
       "        [-1.95840650e-04, -1.03965736e-04,  2.01029616e-04, ...,\n",
       "          5.85495181e-05, -6.78814395e-05,  1.12194932e-04],\n",
       "        [-2.00035982e-04, -1.55969654e-04,  2.09930178e-04, ...,\n",
       "         -4.64093682e-06, -6.84425468e-05,  1.56115668e-04],\n",
       "        ...,\n",
       "        [-1.39643066e-03,  1.30454788e-03,  1.06719614e-03, ...,\n",
       "          2.07928169e-04, -6.25650922e-04, -4.54424007e-04],\n",
       "        [-1.39643066e-03,  1.30454788e-03,  1.06719614e-03, ...,\n",
       "          2.07928169e-04, -6.25650922e-04, -4.54424007e-04],\n",
       "        [-1.39643066e-03,  1.30454788e-03,  1.06719614e-03, ...,\n",
       "          2.07928169e-04, -6.25650922e-04, -4.54424007e-04]],\n",
       "\n",
       "       [[-1.01898862e-04, -2.46928375e-05,  1.25283666e-04, ...,\n",
       "          6.28904236e-05,  1.14771747e-05,  3.32719683e-05],\n",
       "        [-1.17300231e-04, -6.79636855e-07,  1.94247390e-04, ...,\n",
       "          8.52941157e-05, -9.49306195e-06,  1.24954779e-04],\n",
       "        [-1.09051594e-04, -6.16167817e-05,  2.16177985e-04, ...,\n",
       "          6.97152500e-05, -5.10072168e-05,  2.26611388e-04],\n",
       "        ...,\n",
       "        [-1.39643054e-03,  1.30454823e-03,  1.06719614e-03, ...,\n",
       "          2.07928111e-04, -6.25650806e-04, -4.54423862e-04],\n",
       "        [-1.39643054e-03,  1.30454823e-03,  1.06719614e-03, ...,\n",
       "          2.07928111e-04, -6.25650806e-04, -4.54423862e-04],\n",
       "        [-1.39643054e-03,  1.30454823e-03,  1.06719614e-03, ...,\n",
       "          2.07928111e-04, -6.25650806e-04, -4.54423862e-04]],\n",
       "\n",
       "       [[-1.01898862e-04, -2.46928375e-05,  1.25283666e-04, ...,\n",
       "          6.28904236e-05,  1.14771747e-05,  3.32719683e-05],\n",
       "        [-1.76027592e-04,  7.13809004e-06,  4.00997887e-05, ...,\n",
       "          8.55112085e-05, -2.54632978e-05,  6.51385199e-05],\n",
       "        [-1.67990700e-04, -3.56957935e-05,  4.06600748e-06, ...,\n",
       "          7.55394358e-05, -7.30122774e-05,  1.49205676e-04],\n",
       "        ...,\n",
       "        [-1.39643066e-03,  1.30454788e-03,  1.06719614e-03, ...,\n",
       "          2.07928155e-04, -6.25650748e-04, -4.54423920e-04],\n",
       "        [-1.39643066e-03,  1.30454788e-03,  1.06719614e-03, ...,\n",
       "          2.07928155e-04, -6.25650748e-04, -4.54423920e-04],\n",
       "        [-1.39643066e-03,  1.30454788e-03,  1.06719614e-03, ...,\n",
       "          2.07928155e-04, -6.25650748e-04, -4.54423920e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_input, tgt_input in dataset.take(1): break\n",
    "model(src_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-distributor",
   "metadata": {},
   "source": [
    "* 모델의 최종 출력 텐서 shape를 유심히 보면 shape=(256, 20, 7001)  \n",
    "  \n",
    "* 7001은 Dense 레이어의 출력 차원수  \n",
    "* 7001개의 단어 중 어느 단어의 확률이 가장 높을지를 모델링해야 하기 때문  \n",
    "  \n",
    "* 256은 이전 스텝에서 지정한 배치 사이즈  \n",
    "* `dataset.take(1)`를 통해서 1개의 배치, 즉 256개의 문장 데이터를 가져온 것  \n",
    "  \n",
    "* 20은 `tf.keras.layers.LSTM(hidden_size, return_sequences=True)`로 호출한 LSTM 레이어에서 `return_sequences=True`이라고 지정한 부분  \n",
    "* 즉, `LSTM`은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미  \n",
    "* 만약 `return_sequences=False`였다면 `LSTM` 레이어는 1개의 벡터만 출력했을 것  \n",
    "* 그런데 문제는, 우리의 모델은 입력 데이터의 시퀀스 길이가 얼마인지 모름  \n",
    "* 모델을 만들면서 알려준 적도 없음  \n",
    "* 그럼 20은 언제 알게된 것일까?? 데이터를 입력받고 나서...  \n",
    "* 우리 데이터셋의 `max_len`이 20으로 맞춰져 있었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "narrative-craft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  112016    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  20736     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  455065    \n",
      "=================================================================\n",
      "Total params: 620,841\n",
      "Trainable params: 620,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-humanity",
   "metadata": {},
   "source": [
    "* Output Shape를 정확하게 알려주지 않는 이유 : 우리의 모델은 입력 시퀀스의 길이를 모르기 때문에 Output Shape를 특정할 수 없음  \n",
    "* 하지만 모델의 파라미터 사이즈는 측정됨  \n",
    "* 대략 22million 정도  \n",
    "* 참고로 서두에 소개했던 GPT-2의 파라미터 사이즈는, 1.5billion  \n",
    "* GPT-3의 파라미터 사이즈는 GPT-2의 100배"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-print",
   "metadata": {},
   "source": [
    "### (2) 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-detective",
   "metadata": {},
   "source": [
    "* 학습엔 10분 정도 소요(GPU 환경 기준)  \n",
    "* 혹시라도 학습에 지나치게 많은 시간이 소요된다면 `tf.test.is_gpu_available()` 소스를 실행해 텐서플로우가 GPU를 잘 사용하고 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "excessive-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 두 사이트 참고... 메모리 어쩌고 해결 방법이래서 해봤는데 효과는 모르겠다. 우선 여기까지만... 글고 뭔소린지 아직 이해 못함\n",
    "# http://datacrew.tech/tensorflow-2%EC%97%90%EC%84%9C-gpu-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-1with-keras/\n",
    "# https://cnpnote.tistory.com/entry/PYTHON-tensorflow%EC%97%90%EC%84%9C-%ED%98%84%EC%9E%AC-%EC%82%AC%EC%9A%A9-%EA%B0%80%EB%8A%A5%ED%95%9C-GPU%EB%A5%BC-%EC%96%BB%EB%8A%94-%EB%B0%A9%EB%B2%95\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "proof-mouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43937/43937 [==============================] - 854s 19ms/step - loss: 0.1474\n",
      "Epoch 2/10\n",
      "43937/43937 [==============================] - 851s 19ms/step - loss: 0.1277\n",
      "Epoch 3/10\n",
      "43937/43937 [==============================] - 867s 20ms/step - loss: 0.1236\n",
      "Epoch 4/10\n",
      "43937/43937 [==============================] - 866s 20ms/step - loss: 0.1213\n",
      "Epoch 5/10\n",
      "43937/43937 [==============================] - 854s 19ms/step - loss: 0.1199\n",
      "Epoch 6/10\n",
      "43937/43937 [==============================] - 854s 19ms/step - loss: 0.1190\n",
      "Epoch 7/10\n",
      "43937/43937 [==============================] - 854s 19ms/step - loss: 0.1183\n",
      "Epoch 8/10\n",
      "43937/43937 [==============================] - 854s 19ms/step - loss: 0.1178\n",
      "Epoch 9/10\n",
      "43937/43937 [==============================] - 854s 19ms/step - loss: 0.1175\n",
      "Epoch 10/10\n",
      "43937/43937 [==============================] - 854s 19ms/step - loss: 0.1173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18e971d690>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)\n",
    "# 미니배치가 각 배치마다 10에폭씩 실행되면서 업데이트하는 거라고 이해했는데, (그래서 한 에폭당 배치 개수만큼 업데이트??)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-parking",
   "metadata": {},
   "source": [
    "* Loss는 모델이 오답을 만들고 있는 정도라고 생각(그렇다고 Loss가 1일 때 99%를 맞추고 있다는 의미는 아님)  \n",
    "* 오답률이 감소하고 있으니 학습이 잘 진행되고 있다 고 해석 가능!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-nation",
   "metadata": {},
   "source": [
    "### (3) 잘 만들어졌는지 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-adams",
   "metadata": {},
   "source": [
    "* 작문 모델을 평가하는 가장 확실한 방법은 작문을 시켜보고 직접 평가하는 것  \n",
    "* 아래 `generate_text` 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "narrative-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성\n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        # 입력받은 문장의 텐서를 입력\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "        # 모델이 예측한 마지막 단어가 새롭게 생성한 단어가 됨\n",
    "\n",
    "        # 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줌\n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 함\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-measure",
   "metadata": {},
   "source": [
    "* `generate_text()` 함수에서 `init_sentence`를 인자로 받고는 있음   \n",
    "* 이렇게 받은 인자를 일단 텐서로 만들고 있음  \n",
    "* 디폴트로는 `<start>` 단어 하나만 받음  \n",
    "    \n",
    "* `while`의 첫번째 루프에서 `test_tensor`에 `<start>` 하나만 들어갔다고 하고...    \n",
    "* 모델이 출력으로 7001개의 단어 중 A를 골랐다고 할 때...  \n",
    "* `while`의 두번째 루프에서 `test_tensor`에는 `<start> A`가 들어감  \n",
    "* 그래서 모델이 그다음 B를 골랐다고 하면...  \n",
    "* `while`의 세번째 루프에서 `test_tensor`에는 `<start> A B`가 들어감 ......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "painful-borough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m a survivor <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위 문장 생성함수 실행해 보기\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-salvation",
   "metadata": {},
   "source": [
    "* 위 함수의 `init_sentence` 를 바꿔가며 이런저런 실험해 보기  \n",
    "* 단, <start>를 빼먹지 말기!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-narrow",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-order",
   "metadata": {},
   "source": [
    "이번 프로젝트는 앞의 연습을 그대로 따라하기만 해도 코드가 실행되어서 체감 어려움의 정도가 다른 것보다 낮았고 그 점이 너무 좋았다.  \n",
    "하지만 모델 학습 시간이 굉장히 오래 걸려서 약간 마음을 가라앉히는 수행이 필요하였다..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
