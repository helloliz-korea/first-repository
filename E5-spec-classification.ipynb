{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polar-membership",
   "metadata": {},
   "source": [
    "# E5-Spectrogram classification 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-biotechnology",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-palace",
   "metadata": {},
   "source": [
    "### 데이터 다운받기 (1.6G 대용량)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-dinner",
   "metadata": {},
   "source": [
    "mkdir -p ~/aiffel/speech_recognition/data\n",
    "mkdir -p ~/aiffel/speech_recognition/models\n",
    "wget https://aiffelstaticdev.blob.core.windows.net/dataset/speech_wav_8000.npz -P ~/aiffel/speech_recognition/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "documentary-evaluation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave data shape :  (50620, 8000)\n",
      "Label data shape :  (50620, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_path = os.getenv(\"HOME\")+'/aiffel/speech_recognition/data/speech_wav_8000.npz'\n",
    "speech_data = np.load(data_path)\n",
    "\n",
    "print(\"Wave data shape : \", speech_data[\"wav_vals\"].shape)\n",
    "print(\"Label data shape : \", speech_data[\"label_vals\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-mathematics",
   "metadata": {},
   "source": [
    "### 데이터 확인해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threaded-assurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand num :  19067\n",
      "Wave data shape :  (8000,)\n",
      "label :  ['up']\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "import random\n",
    "\n",
    "# 데이터 선택 (랜덤하게 선택하고 있으니, 여러번 실행해 보세요)\n",
    "rand = random.randint(0, len(speech_data[\"wav_vals\"]))\n",
    "print(\"rand num : \", rand)\n",
    "\n",
    "sr = 8000 # 1초동안 재생되는 샘플의 갯수\n",
    "data = speech_data[\"wav_vals\"][rand]  # 웨이브 데이터!!!!!! 여기에 저장돼 있음!!!!!!\n",
    "print(\"Wave data shape : \", data.shape)\n",
    "print(\"label : \", speech_data[\"label_vals\"][rand])\n",
    "\n",
    "#ipd.Audio(data, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-delay",
   "metadata": {},
   "source": [
    "### Spectrogram 으로 변환 후, 입력 데이터에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-petroleum",
   "metadata": {},
   "source": [
    "* wav 데이터를 해석하는 방법 중 하나로, 일정 시간동안 wav 데이터 안의 다양한 주파수들이 얼마나 포함되어 있는 지를 보여줌  \n",
    "* X축은 시간, Y축은 주파수  \n",
    "* 해당 시간/주파수에서의 음파 강도에 따라 밝은색으로 표현  \n",
    "* wav 데이터가 단위 시간만큼 Short Time Fourier Transform을 진행해 매 순간의 주파수 데이터들을 얻어서 Spectrogram을 완성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-patrick",
   "metadata": {},
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aboriginal-while",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "def wav2spec(wav, fft_size=258): # spectrogram shape을 맞추기위해서 size 변형\n",
    "    D = np.abs(librosa.stft(wav, n_fft=fft_size))\n",
    "    return D\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "still-error",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform shape :  (8000,)\n",
      "Spectrogram shape :  (130, 126)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "spec = wav2spec(data) # 웨이브 데이터를 스펙토그램으로 변환 (1-dim data -> 2-dim data)\n",
    "print(\"Waveform shape : \",data.shape)\n",
    "print(\"Spectrogram shape : \",spec.shape)\n",
    "print(type(spec))\n",
    "\n",
    "spec = np.array(spec, np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-enemy",
   "metadata": {},
   "source": [
    "* data = speech_data[\"wav_vals\"][rand] 위의 결과와 아래의 결과가 같은데, 이렇게 할 경우 스펙데이터의 길이와 라벨데이터의 길이가 달라서 학습데이터와 평가데이터 분리가 안 된다. 뭘 어쩌라는 건지...\n",
    "\n",
    "spec = wav2spec(speech_data[\"wav_vals\"][0])\n",
    "print(\"Waveform shape : \",speech_data[\"wav_vals\"][0].shape)\n",
    "print(\"Spectrogram shape : \",spec.shape)\n",
    "print(type(spec))\n",
    "\n",
    "spec = np.array(spec, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "technical-rocket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 130, 126)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_data = []\n",
    "for wav in data:\n",
    "    spec = wav2spec(data)\n",
    "    spec_data.append(spec)\n",
    "    \n",
    "spec_data = np.array(spec_data)\n",
    "spec_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-pitch",
   "metadata": {},
   "source": [
    "### 라벨 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "official-remainder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL :  ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence']\n",
      "Indexed LABEL :  {'yes': 0, 'no': 1, 'up': 2, 'down': 3, 'left': 4, 'right': 5, 'on': 6, 'off': 7, 'stop': 8, 'go': 9, 'unknown': 10, 'silence': 11}\n"
     ]
    }
   ],
   "source": [
    "target_list = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "\n",
    "label_value = target_list\n",
    "label_value.append('unknown')\n",
    "label_value.append('silence')\n",
    "\n",
    "print('LABEL : ', label_value)\n",
    "\n",
    "new_label_value = dict()\n",
    "for i, l in enumerate(label_value):\n",
    "    new_label_value[l] = i\n",
    "    \n",
    "label_value = new_label_value\n",
    "\n",
    "print('Indexed LABEL : ', new_label_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "billion-rotation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  3,  3, ..., 11, 11, 11])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for v in speech_data[\"label_vals\"]:\n",
    "    temp.append(label_value[v[0]])\n",
    "label_data = np.array(temp)\n",
    "\n",
    "label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alternate-democracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "50620\n"
     ]
    }
   ],
   "source": [
    "print(len(spec_data))\n",
    "print(len(label_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-diana",
   "metadata": {},
   "source": [
    "### 학습을 위한 데이터 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-newton",
   "metadata": {},
   "source": [
    "* sklearn의 train_test_split 함수 이용 : train data와 test data 분리  \n",
    "* test_size의 인자를 조절해주면, 설정해 준 값만큼 Test dataset의 비율을 조정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(spec_data)와 len(label_data)가 달라서 오류가 생기는데 뭘 어쩌라는 건지 모르겠다... 난 최선을 다했음.......\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sr = 130\n",
    "sc = 126\n",
    "\n",
    "train_spec, test_spec, train_label, test_label = train_test_split(spec_data, \n",
    "                                                                label_data, \n",
    "                                                                test_size=0.1,\n",
    "                                                                shuffle=True)\n",
    "#print(train_wav)\n",
    "\n",
    "train_spec = train_spec.reshape([-1, sr, sc,1]) # add channel for CNN\n",
    "test_spec = test_spec.reshape([-1, sr, sc,1])\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-andorra",
   "metadata": {},
   "source": [
    "* 분리된 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train data : \", train_spec.shape)\n",
    "print(\"train labels : \", train_label.shape)\n",
    "print(\"test data : \", test_spec.shape)\n",
    "print(\"test labels : \", test_label.shape)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-defeat",
   "metadata": {},
   "source": [
    "### Hyper-parameters setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-member",
   "metadata": {},
   "source": [
    "* 학습을 위한 하이퍼파라미터 설정  \n",
    "* 모델 체크포인트 저장을 위한 체크포인트의 경로 설정  \n",
    "* 후에 모델 체크포인트 Callback 함수를 설정하거나, 모델을 불러올 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 10\n",
    "\n",
    "# the save point\n",
    "checkpoint_dir = os.getenv('HOME')+'/aiffel/speech_recognition/models/wav'\n",
    "\n",
    "checkpoint_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-updating",
   "metadata": {},
   "source": [
    "### Data setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-escape",
   "metadata": {},
   "source": [
    "* `tf.data.Dataset`을 이용하여 데이터셋 구성 (`Tensorflow`에 포함된 데이터셋 관리 패키지)  \n",
    "* 위 패키지는 데이터셋 전처리, 배치처리 등을 쉽게 할 수 있도록 해 줌  \n",
    "* `tf.data.Dataset.from_tensor_slices` 함수에 return 받길 원하는 데이터를 튜플 (data, label) 형태로 넣어서 사용  \n",
    "  \n",
    "* map 함수는 dataset이 데이터를 불러올때마다 동작시킬 데이터 전처리 함수를 매핑해 주는 역할  \n",
    "* 첫번째 map 함수는 `from_tensor_slice` 에 입력한 튜플 형태로 데이터를 받으며 return 값으로 어떤 데이터를 반환할지 결정  \n",
    "* map 함수는 중첩해서 사용 가능  \n",
    "  \n",
    "* 아래와 같이, map 함수에 넘겨줄 데이터 전처리 함수를 작성해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_label(wav, label):\n",
    "    label = tf.one_hot(label, depth=12)\n",
    "    return wav, label\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-broadcasting",
   "metadata": {},
   "source": [
    "* `tf.data.Dataset` 함수 구성  \n",
    "* `batch`는 dataset에서 제공하는 튜플 형태의 데이터를 얼마나 가져올지 결정하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_spec, train_label))\n",
    "train_dataset = train_dataset.map(one_hot_label)\n",
    "train_dataset = train_dataset.repeat().batch(batch_size=batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_spec, test_label))\n",
    "test_dataset = test_dataset.map(one_hot_label)\n",
    "test_dataset = test_dataset.batch(batch_size=batch_size)\n",
    "print(test_dataset)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-crisis",
   "metadata": {},
   "source": [
    "## 2. 스펙토그램 classification 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-government",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "input_tensor = layers.Input(shape=(sr, sc, 1))  # sc 추가해서 차원 맞춰주고\n",
    "\n",
    "x = layers.Conv2D(32, (3,3), padding='same', activation='relu')(input_tensor)  # Conv1D는 Conv2D로, 9는 (3,3)으로 변경\n",
    "x = layers.Conv2D(32, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(128, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(128, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(128, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(256, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(256, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(256, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "output_tensor = layers.Dense(12)(x)\n",
    "\n",
    "model_wav = tf.keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "model_wav.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-interaction",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-forge",
   "metadata": {},
   "source": [
    "* 현재 라벨이 될 수 있는 12개의 단어 class 가지고 있음  \n",
    "* 해당 class를 구분하기 위해 `multi-class classification` 필요  \n",
    "* 이를 수행하기 위한 `Loss`로 `Categorical Cross-Entropy loss`를 사용할 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    "model_wav.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "             optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-assets",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-alias",
   "metadata": {},
   "source": [
    "#### Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-drawing",
   "metadata": {},
   "source": [
    "* `model.fit` 함수를 이용할 때, `callback` 함수를 이용하여 학습 중간 중간 원하는 동작을 하도록 설정 가능  \n",
    "* 모델을 재사용하기 위해 모델 가중치를 저장하는 `callback` 함수 추가  \n",
    "  \n",
    "* `Model Checkpoint callback`은 모델을 학습을 진행하며, `fit` 함수 내 다양한 인자를 지정해 모니터하며 동작하게 설정 가능  \n",
    "* 현재 모델은 `validation loss`를 모니터하며, `loss`가 낮아지면 모델 파라미터를 저장하도록 구성되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 mode='auto',\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-legislature",
   "metadata": {},
   "source": [
    "* 아래는 모델 학습 코드  \n",
    "* 이전 스텝의 하이퍼파라미터 세팅에서 `batch_size=32, max_epochs=10`으로 세팅한 경우라면 30분 가량 소요될 것  \n",
    "* 메모리 사용량에 주의하며 적절히 하이퍼파라미터 세팅을 조절  \n",
    "* 메모리가 부족하다면 batch_size를 작게 조절해 주는게 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30분 내외 소요 (메모리 사용량에 주의해 주세요.)\n",
    "history_wav = model_wav.fit(train_dataset, epochs=max_epochs,\n",
    "                    steps_per_epoch=len(train_spec) // batch_size,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=len(test_spec) // batch_size,\n",
    "                    callbacks=[cp_callback]\n",
    "                    )\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-enhancement",
   "metadata": {},
   "source": [
    "### 학습 결과 Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-measure",
   "metadata": {},
   "source": [
    "* `model.fit` 함수는 학습 동안의 결과를 return  \n",
    "* return 값을 기반으로 loss와 accuracy를 그래프로 표현  \n",
    "* `fit` 함수에서 전달 받은 Loss와 Accuracy의 값을 이용해 모델이 어떻게 학습되고 있는지 확인 가능  \n",
    "* `train loss`와 `val_loss`의 차이가 커지는 경우 오버피팅이 일어나는 것이기 때문에 이를 수정할 필요가 있음  \n",
    "  \n",
    "* 출력된 그래프를 기반으로 모델의 학습이 어떻게 진행됐는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_wav.history['accuracy']\n",
    "val_acc = history_wav.history['val_accuracy']\n",
    "\n",
    "loss=history_wav.history['loss']\n",
    "val_loss=history_wav.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-variety",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-aspect",
   "metadata": {},
   "source": [
    "* Test dataset을 이용해서 모델의 성능 평가  \n",
    "  \n",
    "* 실습삼아 `checkpoint callback` 함수가 저장한 weight를 다시 불러와서 테스트 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wav.load_weights(checkpoint_dir)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-consumer",
   "metadata": {},
   "source": [
    "* Test data을 이용하여 모델의 예측값과 실제값이 얼마나 일치하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_wav.evaluate(test_dataset)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "print(\"loss value: {:.3f}\".format(results[0]))\n",
    "# accuracy\n",
    "print(\"accuracy value: {:.4f}%\".format(results[1]*100))\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-storm",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-personality",
   "metadata": {},
   "source": [
    "* Test data 셋을 골라 직접 들어보고 모델의 예측이 맞는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_label_value = {v: k for k, v in label_value.items()}\n",
    "batch_index = np.random.choice(len(test_wav), size=1, replace=False)\n",
    "\n",
    "batch_xs = test_wav[batch_index]\n",
    "batch_ys = test_label[batch_index]\n",
    "y_pred_ = model_wav(batch_xs, training=False)\n",
    "\n",
    "print(\"label : \", str(inv_label_value[batch_ys[0]]))\n",
    "\n",
    "ipd.Audio(batch_xs.reshape(8000,), rate=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-consciousness",
   "metadata": {},
   "source": [
    "* 위에서 확인해본 테스트셋의 라벨과 우리 모델의 실제 prediction 결과를 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.argmax(y_pred_) == batch_ys[0]:\n",
    "    print(\"y_pred: \" + str(inv_label_value[np.argmax(y_pred_)]) + '(Correct!)')\n",
    "else:\n",
    "    print(\"y_pred: \" + str(inv_label_value[np.argmax(y_pred_)]) + '(Incorrect!)')\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-pontiac",
   "metadata": {},
   "source": [
    "## 3. Skip-Connection model을 추가해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-coach",
   "metadata": {},
   "source": [
    "### Skip-Connection model 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-nothing",
   "metadata": {},
   "source": [
    "* 기존의 모델을 `skip-connection`이 추가된 모델로 변경해 학습을 진행  \n",
    "* 위쪽의 데이터가 레이어를 뛰어넘어 레이어를 통과한 값에 더해주는 형식으로 구현  \n",
    "* Concat을 이용한 방식으로 구현  \n",
    "  \n",
    "* `tf.concat([#layer output tensor, layer output tensor#], axis=#)`  \n",
    "  \n",
    "* 우리가 사용하는 데이터가 1차원 audio 데이터이기 때문에 1차원 데이터를 처리하는 모델을 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = layers.Input(shape=(sr, sc, 1))  # 여기 마찬가지로 sc 추가\n",
    "\n",
    "x = layers.Conv2D(32, (3,3), padding='same', activation='relu')(input_tensor) # Conv1D는 Conv2D로, 9는 (3,3)으로 변경\n",
    "x = layers.Conv2D(32, (3,3), padding='same', activation='relu')(x)\n",
    "skip_1 = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(64, (3,3), padding='same', activation='relu')(skip_1)\n",
    "x = layers.Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "x = tf.concat([x, skip_1], -1)\n",
    "skip_2 = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(128, (3,3), padding='same', activation='relu')(skip_2)\n",
    "x = layers.Conv2D(128, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(128, (3,3), padding='same', activation='relu')(x)\n",
    "x = tf.concat([x, skip_2], -1)\n",
    "skip_3 = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(256, (3,3), padding='same', activation='relu')(skip_3)\n",
    "x = layers.Conv2D(256, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(256, (3,3), padding='same', activation='relu')(x)\n",
    "x = tf.concat([x, skip_3], -1)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "output_tensor = layers.Dense(12)(x)\n",
    "\n",
    "model_wav_skip = tf.keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "model_wav_skip.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-reason",
   "metadata": {},
   "source": [
    "* 모델 구성만 달라졌을 뿐, 그 외 Task구성이나 데이터셋 구성, 훈련 과정은 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    "model_wav_skip.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "             optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the save point\n",
    "checkpoint_dir = os.getenv('HOME')+'/aiffel/speech_recognition/models/wav_skip'\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 mode='auto',\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30분 내외 소요\n",
    "history_wav_skip = model_wav_skip.fit(train_dataset, epochs=max_epochs,\n",
    "                    steps_per_epoch=len(train_wav) // batch_size,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=len(test_wav) // batch_size,\n",
    "                    callbacks=[cp_callback]\n",
    "                    )\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-pearl",
   "metadata": {},
   "source": [
    "### 학습결과의 시각화 및 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_wav_skip.history['accuracy']\n",
    "val_acc = history_wav_skip.history['val_accuracy']\n",
    "\n",
    "loss=history_wav_skip.history['loss']\n",
    "val_loss=history_wav_skip.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "\n",
    "model_wav_skip.load_weights(checkpoint_dir)\n",
    "results = model_wav_skip.evaluate(test_dataset)\n",
    "\n",
    "# loss\n",
    "print(\"loss value: {:.3f}\".format(results[0]))\n",
    "# accuracy\n",
    "print(\"accuracy value: {:.4f}%\".format(results[1]*100))\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "\n",
    "inv_label_value = {v: k for k, v in label_value.items()}\n",
    "batch_index = np.random.choice(len(test_wav), size=1, replace=False)\n",
    "\n",
    "batch_xs = test_wav[batch_index]\n",
    "batch_ys = test_label[batch_index]\n",
    "y_pred_ = model_wav_skip(batch_xs, training=False)\n",
    "\n",
    "print(\"label : \", str(inv_label_value[batch_ys[0]]))\n",
    "\n",
    "ipd.Audio(batch_xs.reshape(8000,), rate=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-penetration",
   "metadata": {},
   "source": [
    "### 위에서 확인해본 테스트셋의 라벨과 우리 모델의 실제 prediction 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.argmax(y_pred_) == batch_ys[0]:\n",
    "    print(\"y_pred: \" + str(inv_label_value[np.argmax(y_pred_)]) + '(Correct!)')\n",
    "else:\n",
    "    print(\"y_pred: \" + str(inv_label_value[np.argmax(y_pred_)]) + '(Incorrect!)')\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-vehicle",
   "metadata": {},
   "source": [
    "## 4. 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-prefix",
   "metadata": {},
   "source": [
    "데이터 준비하기부터 이해가 안 가서 전 과정이 이해가 안 간다.  \n",
    "1차원 데이터를 2차원 데이터로 변경한 후 분류하는 과정이라고 이해했는데 대체 데이터 수는 어떻게 결정되는 것인지도 모르겠고, 전반적인 이해도 프로젝트를 고민할 시간도 많이 부족했던 것 같다.  \n",
    "다시 만나고 싶지 않은 음성데이터..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
