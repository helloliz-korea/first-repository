{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "experienced-evanescence",
   "metadata": {},
   "source": [
    "# E-11-(연습) 뉴스 요약봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-baltimore",
   "metadata": {},
   "source": [
    "## 텍스트 요약(Text Summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-palmer",
   "metadata": {},
   "source": [
    "* 추출적 요약(Extractive Summarization) : 원문에서 문장들을 추출해서 요약하는 방식  \n",
    "* 추상적 요약(Abstractive Summarization) : 원문으로부터 내용이 요약된 새로운 문장을 생성해내는 것, 새로운 문장이라는 것은 결과로 나온 문장이 원문에 원래 없던 문장일 수도 있다는 것을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-conservative",
   "metadata": {},
   "source": [
    "## 인공 신경망으로 텍스트 요약 훈련시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-mixer",
   "metadata": {},
   "source": [
    "* seq2seq 모델을 이용하여 Abstractive summarization 방식의 텍스트 요약기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-abortion",
   "metadata": {},
   "source": [
    "* seq2seq  \n",
    "* LSTM과 컨텍스트 벡터 : seq2seq 구현 시, 인코더/디코더로 바닐라 RNN이 아닌 LSTM 사용  \n",
    "* 시작 토큰과 종료 토큰 : 훈련 데이터의 예측 대상 시퀀스의 앞/뒤에는 시작 토큰과 종료 토큰을 넣어주는 전처리를 통해 어디에서 멈춰야하는지 알려줘야 함  \n",
    "* 어텐션 메커니즘을 통한 새로운 컨텍스트 벡터 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-johnson",
   "metadata": {},
   "source": [
    "##### 위 내용 요약 정리  \n",
    "   1. seq2seq를 사용합니다.  \n",
    "   2. RNN 계열 중 LSTM을 사용하므로 hidden state뿐만 아니라 cell state도 사용해야 합니다.  \n",
    "   3. 디코더의 예측 시퀀스에는 시작 토큰 SOS와 예측 토큰 EOS를 시퀀스의 앞, 뒤로 붙입니다.  \n",
    "   4. seq2seq를 구동시키면 디코더는 시작 토큰을 입력받아 예측을 시작합니다.  \n",
    "   5. seq2seq 기본 모델과 달리, 어텐션 메커니즘을 이용해 인코더의 hidden state의 중요도를 취합한 컨텍스트 벡터를 디코더 스텝별로 계산합니다.  \n",
    "   6. 계산된 컨텍스트 벡터를 이용해서 디코더는 다음 등장할 단어를 예측합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-corrections",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-lemon",
   "metadata": {},
   "source": [
    "* 아마존 리뷰 데이터셋 다운로드(`Reviews.csv`) : https://www.kaggle.com/snap/amazon-fine-food-reviews  \n",
    "* NLTK의 불용어(stopwords)를 사용 : NTLK와 NLTK 데이터셋이 설치되어있지 않은 환경이라면 NLTK를 설치하고 NTLK의 데이터셋을 다운로드\n",
    "* NTLK 다운로드 : `pip install nltk`  \n",
    "* BeautifulSoup 라이브러리(문서를 파싱하는데 사용하는 패키지) 설치 : `pip install beautifulsoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "weird-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aiffel-\n",
      "[nltk_data]     dj42/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk 패키지에서 불용어 사전을 다운로드 받고, 데이터 전처리를 위한 나머지 패키지도 함께 불러오기\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stunning-shannon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 100000\n"
     ]
    }
   ],
   "source": [
    "# 링크에서 다운로드 받은 데이터(Reviews.csv)에는 총 568,454개의 샘플 있음\n",
    "# 시간상 모든 샘플을 사용하지는 않고, 10만개만 사용해볼 것임\n",
    "# 주의!! : 데이터 파일의 경로를 수정하는 것 잊지 말기\n",
    "\n",
    "data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/news_summarization/data/Reviews.csv\", nrows = 100000)\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infrared-helen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 십만 개의 샘플 중 5개만 출력해보기\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comprehensive-salon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96985</th>\n",
       "      <td>Like many moms, I have discovered that my todd...</td>\n",
       "      <td>happy to let my kid eat this by the spoonful!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75531</th>\n",
       "      <td>I discovered this product in 2011. Loacker Qua...</td>\n",
       "      <td>Absolutely delicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62188</th>\n",
       "      <td>Royal Oak Virginia Crabbers are a great produc...</td>\n",
       "      <td>Last year's crop!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23224</th>\n",
       "      <td>You either love em or hate em.  I personally l...</td>\n",
       "      <td>I love me some lemonheads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17452</th>\n",
       "      <td>This is one of my favorite bold coffees.  It i...</td>\n",
       "      <td>Excellent Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31322</th>\n",
       "      <td>I got a similar variety pack at Costco and eve...</td>\n",
       "      <td>Delicious!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33907</th>\n",
       "      <td>I have used this product for quite some time o...</td>\n",
       "      <td>It works!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>service was great, but I meant to buy Swiss Ch...</td>\n",
       "      <td>Swiss Chalet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29707</th>\n",
       "      <td>I discovered Bigelow's Vanilla Carmel tea a co...</td>\n",
       "      <td>Fantastic hot or iced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963</th>\n",
       "      <td>My dogs love these and even my allergy dogs ca...</td>\n",
       "      <td>Old Mother Hubbard Crunchy Classic Snacks for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48663</th>\n",
       "      <td>I got the Quaker Banana Nut bar in my Influens...</td>\n",
       "      <td>Good snack!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81609</th>\n",
       "      <td>These almonds just have a great flavor.  Just ...</td>\n",
       "      <td>tasty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71524</th>\n",
       "      <td>Although I don't like dark chocolate, I have t...</td>\n",
       "      <td>5++ stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75810</th>\n",
       "      <td>Let me preface this review by saying that I'm ...</td>\n",
       "      <td>Not bad.  Not bad at all.  In fact, pretty good.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45948</th>\n",
       "      <td>Decent-tasting beverage.&lt;br /&gt;&lt;br /&gt;Personally...</td>\n",
       "      <td>Gold Kili All Natural Instant Ginger &amp; Lemon B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "96985  Like many moms, I have discovered that my todd...   \n",
       "75531  I discovered this product in 2011. Loacker Qua...   \n",
       "62188  Royal Oak Virginia Crabbers are a great produc...   \n",
       "23224  You either love em or hate em.  I personally l...   \n",
       "17452  This is one of my favorite bold coffees.  It i...   \n",
       "31322  I got a similar variety pack at Costco and eve...   \n",
       "33907  I have used this product for quite some time o...   \n",
       "318    service was great, but I meant to buy Swiss Ch...   \n",
       "29707  I discovered Bigelow's Vanilla Carmel tea a co...   \n",
       "5963   My dogs love these and even my allergy dogs ca...   \n",
       "48663  I got the Quaker Banana Nut bar in my Influens...   \n",
       "81609  These almonds just have a great flavor.  Just ...   \n",
       "71524  Although I don't like dark chocolate, I have t...   \n",
       "75810  Let me preface this review by saying that I'm ...   \n",
       "45948  Decent-tasting beverage.<br /><br />Personally...   \n",
       "\n",
       "                                                 Summary  \n",
       "96985      happy to let my kid eat this by the spoonful!  \n",
       "75531                               Absolutely delicious  \n",
       "62188                                  Last year's crop!  \n",
       "23224                         I love me some lemonheads.  \n",
       "17452                                   Excellent Coffee  \n",
       "31322                                     Delicious!!!!!  \n",
       "33907                                          It works!  \n",
       "318                                         Swiss Chalet  \n",
       "29707                              Fantastic hot or iced  \n",
       "5963   Old Mother Hubbard Crunchy Classic Snacks for ...  \n",
       "48663                                        Good snack!  \n",
       "81609                                              tasty  \n",
       "71524                                          5++ stars  \n",
       "75810   Not bad.  Not bad at all.  In fact, pretty good.  \n",
       "45948  Gold Kili All Natural Instant Ginger & Lemon B...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 데이터 중 필요한 Summary열과 Text열만 훈련에 사용, 이 두 개의 열만 별도로 저장하고 다시 출력하기\n",
    "\n",
    "data = data[['Text','Summary']]\n",
    "data.head()\n",
    "\n",
    "#랜덤한 15개 샘플 출력\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-corps",
   "metadata": {},
   "source": [
    "* 이 실습에선 인공 신경망을 통해 Text 시퀀스를 입력받으면, Summary 시퀀스를 예측하도록 인공 신경망을 훈련시킬 것임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-postcard",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-powder",
   "metadata": {},
   "source": [
    "### (1) 데이터 정리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-suggestion",
   "metadata": {},
   "source": [
    "#### 중복 샘플과 NULL 값이 존재하는 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gorgeous-segment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 중복 샘플 유무 확인\n",
    "\n",
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())\n",
    "\n",
    "# Text가 달라도 Summary는 동일할 수 있음\n",
    "# 하지만 Text 자체가 중복된 경우는 중복 샘플이므로 제거해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "smooth-devices",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88426\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임의 drop_duplicates()를 사용하여 중복 샘플 제거\n",
    "\n",
    "data.drop_duplicates(subset = ['Text'], inplace = True)\n",
    "print('전체 샘플수 :',(len(data)))\n",
    "\n",
    "# 그런데 만약 데이터 Null값을 가지는 샘플이 있었다면, drop_duplicates()가 중복된 Null들을 지워주기는 하겠지만,\n",
    "# 여전히 Null값 한개가 어딘가 남아있을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "private-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임에 Null 값이 있는지 확인 : .isnull().sum()을 사용\n",
    "\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "olympic-trance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임에서 Null 제거 : dropna() 함수 사용\n",
    "\n",
    "data.dropna(axis = 0, inplace = True)\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-acceptance",
   "metadata": {},
   "source": [
    "#### 텍스트 정규화와 불용어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-vertical",
   "metadata": {},
   "source": [
    "* 텍스트 정규화(text normalization) : 단어들 중 같은 의미 다른 표현들을 (예 : it'll/it will, mustn't/must not) 기계 학습 전에 미리 같은 표현으로 통일시켜 기계의 연산량 줄이기  \n",
    "* 정규화 사전 출처 : https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "phantom-transcription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 정규화를 위한 사전(dictionary) 구성\n",
    "\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \",len(contractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "executive-former",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# 샘플에서 불용어를 제거 : NLTK에서 제공하는 불용어 리스트를 참조\n",
    "\n",
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "intermediate-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\",sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "decent-playing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything bought great infact ordered twice third ordered wasfor mother father\n",
      "great way to start the day\n"
     ]
    }
   ],
   "source": [
    "# 전처리 전, 후의 결과를 확인하기 위해 임의의 text와 summary를 만들어 함수를 호출\n",
    "# (코드 오류가 난다면 parser가 설치되어있지 않은 것이니, lxml을 설치 : pip install lxml)\n",
    "\n",
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(preprocess_sentence(temp_text))\n",
    "print(preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다.\n",
    "\n",
    "# 결과 : 모두 소문자로 변환, html 태그 제거, 괄호나 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "capable-australian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
       " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo',\n",
       " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch',\n",
       " 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal',\n",
       " 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터 전체에 전처리 수행\n",
    "# 이때, Text의 경우 불용어 제거하고, Summary의 경우 불용어를 제거하지 않을 것이므로 따로 호출하여 진행\n",
    "# 먼저 Text를 전처리하고, 결과를 확인하기 위해서 상위 5개의 줄을 출력해보기\n",
    "\n",
    "clean_text = []\n",
    "\n",
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "# 전처리 후 출력\n",
    "clean_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "signal-arctic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj42/anaconda3/envs/aiffel/lib/python3.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['good quality dog food',\n",
       " 'not as advertised',\n",
       " 'delight says it all',\n",
       " 'cough medicine',\n",
       " 'great taffy']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이제 Summary에 대해서 전처리 함수를 호출해줄 때, 불용어 제거를 수행하지 않는다는 의미에서 두번째 인자로 False를 넣어주기\n",
    "\n",
    "clean_summary = []\n",
    "\n",
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Summary']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "clean_summary[:5]\n",
    "\n",
    "# 텍스트 정제 과정을 거친 후, 다시 한번 빈(empty) 샘플이 생겼는지 확인\n",
    "# 정제 전에는 데이터가 존재했지만, 정제 과정에서 문장의 모든 단어가 사라지는 경우가 있을 수 있음\n",
    "# 이렇게 되면 샘플 자체가 빈 값을 가지게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "still-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쉽게 확인하기 위해 데이터들을 데이터프레임에 재저장하고 빈(empty) 값을 갖는 샘플들은 모두 Null 값을 가진 샘플로 대체\n",
    "\n",
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "italic-underwear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Summary    70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .isnull().sum()을 사용하여 Null 값이 생겼는지 확인\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "# Summary 열에서 70개의 Null 값이 생성\n",
    "# 원래는 단어가 있었는데, 정제 과정에서 모든 단어가 제거되어 빈 샘플이 70개 생겼다는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "similar-advocate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88355\n"
     ]
    }
   ],
   "source": [
    "# Null 샘플들 모두 제거\n",
    "\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :',(len(data)))  #데이터 전처리 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-retrieval",
   "metadata": {},
   "source": [
    "### (2) 훈련데이터와 테스트데이터 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-miracle",
   "metadata": {},
   "source": [
    "* 학습에 사용할 데이터의 크기를 결정하고, 문장의 시작과 끝 표시하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-rainbow",
   "metadata": {},
   "source": [
    "#### 샘플의 최대 길이 정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "extreme-booking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 2\n",
      "텍스트의 최대 길이 : 1235\n",
      "텍스트의 평균 길이 : 38.792428272310566\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 28\n",
      "요약의 평균 길이 : 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text와 Summary의 최소, 최대, 평균 길이를 구하고 길이 분포를 시각화하여 확인\n",
    "\n",
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-prize",
   "metadata": {},
   "source": [
    "위의 그래프처럼, 많은 양의 데이터를 다룰때는 데이터를 시각화하여 보는 것이 도움이 돼요. 위에서 부터 차례대로 그래프는 각각 요약과 실제 텍스트의 길이 분포, 요약본 샘플 길이별 갯수, 실제 텍스트 샘플 길이별 갯수를 나타내고 있어요.  \n",
    "  \n",
    "Text의 경우 최소 길이가 2, 최대 길이가 1,235으로 그 차이가 굉장히 크죠. 하지만 평균 길이는 38로 시각화 된 그래프로 봤을 때는 대체적으로는 100 내외의 길이를 가진다는 것을 확인할 수 있어요.  \n",
    "  \n",
    "Summary의 경우 최소 길이가 1, 최대 길이가 28, 그리고 평균 길이가 4로 Text에 비해 상대적으로 길이가 매우 짧아요. 그래프로 봤을 때에도 대체적으로 10이하의 길이를 가지고 있네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "continuing-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text의 최대 길이와 Summary의 적절한 최대 길이 임의로 정해보기\n",
    "\n",
    "text_max_len = 50\n",
    "summary_max_len = 8\n",
    "\n",
    "# 이 길이를 선택했을 때, 얼마나 많은 샘플들을 자르지 않고 포함할 수 있는지 통계로 확인하는 편이 객관적으로 길이를 결정하는데 도움이 될 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "proved-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좀 더 정확한 판단을 위하여, 훈련 데이터와 샘플의 길이를 입력하면 데이터의 몇 %가 해당하는지 계산하는 함수 만들기\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "smooth-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7745119121724859\n",
      "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "# 함수를 Text와 Summary에 적용해 우리가 결정한 임의의 길이가 몇%의 샘플까지 포함하는지 확인해 보기\n",
    "\n",
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])\n",
    "\n",
    "# 결과 : 각각 50과 8로 패딩을 하게되면 해당 길이보다 긴 샘플들은 내용이 잘리게 되는데,\n",
    "# Text 열의 경우에는 약 23%의 샘플들이 내용이 망가지게 된다함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "damaged-tournament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 65818\n"
     ]
    }
   ],
   "source": [
    "# 정해진 길이에 맞춰 자르는 것이 아니라, 정해진 길이보다 길면 제외하는 방법으로 데이터를 정제\n",
    "\n",
    "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-donna",
   "metadata": {},
   "source": [
    "#### 시작 토큰과 종료 토큰 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sublime-morrison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>sostoken good quality dog food</td>\n",
       "      <td>good quality dog food eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>sostoken not as advertised</td>\n",
       "      <td>not as advertised eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>sostoken delight says it all</td>\n",
       "      <td>delight says it all eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>sostoken cough medicine</td>\n",
       "      <td>cough medicine eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>sostoken great taffy</td>\n",
       "      <td>great taffy eostoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary  \\\n",
       "0  bought several vitality canned dog food produc...  good quality dog food   \n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised   \n",
       "2  confection around centuries light pillowy citr...    delight says it all   \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine   \n",
       "4  great taffy great price wide assortment yummy ...            great taffy   \n",
       "\n",
       "                    decoder_input                  decoder_target  \n",
       "0  sostoken good quality dog food  good quality dog food eostoken  \n",
       "1      sostoken not as advertised      not as advertised eostoken  \n",
       "2    sostoken delight says it all    delight says it all eostoken  \n",
       "3         sostoken cough medicine         cough medicine eostoken  \n",
       "4            sostoken great taffy            great taffy eostoken  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰은 'sostoken', 종료 토큰은 'eostoken'이라 임의로 명명하고 앞, 뒤로 추가\n",
    "# 디코더의 입력에 해당하면서 시작 토큰이 맨 앞에 있는 문장의 이름을 decoder_input,\n",
    "# 디코더의 출력 또는 레이블에 해당되면서 종료 토큰이 맨 뒤에 붙는 문장의 이름을 decoder_target이라고 하기\n",
    "# 두 개의 문장 모두 Summary 열로부터 만들 것임\n",
    "\n",
    "#요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "unique-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더의 입력, 디코더의 입력과 레이블을 각각 다시 Numpy 타입으로 저장\n",
    "\n",
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "looking-evidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22351 42100 29806 ... 42115  5489  3257]\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터와 테스트 데이터 분리 : 분리 패키지를 사용하는 방법과 직접 코딩을 통해서 분리하는 방법 중 직접 코딩하여 분리\n",
    "\n",
    "# 1. encoder_input과 크기 및 형태가 같은 순서가 섞인 정수 시퀀스 만들기\n",
    "\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "collected-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 정수 시퀀스를 이용해 다시 데이터의 샘플 순서를 정의해주어 잘 섞인 샘플 만들기\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "twenty-survival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 13163\n"
     ]
    }
   ],
   "source": [
    "# 섞인 데이터를 8:2의 비율로 훈련 데이터와 테스트 데이터로 분리\n",
    "\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :',n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "primary-doctrine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 52655\n",
      "훈련 레이블의 개수 : 52655\n",
      "테스트 데이터의 개수 : 13163\n",
      "테스트 레이블의 개수 : 13163\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터의 개수를 이용하여 전체 데이터 분할\n",
    "# :표시의 위치에 주의\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-computer",
   "metadata": {},
   "source": [
    "### (3) 정수 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-scroll",
   "metadata": {},
   "source": [
    "#### 단어 집합(vocaburary) 만들기 및 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "artistic-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터에 대해서 단어 집합을 만들어보기\n",
    "# 우선, 원문에 해당되는 encoder_input_train에 대해서 단어 집합을 만들기\n",
    "# Keras의 토크나이저를 사용하면, 입력된 훈련 데이터로부터 단어 집합을 만들 수 있음\n",
    "\n",
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n",
    "\n",
    "# 이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수 부여\n",
    "# 현재 생성된 단어 집합은 src_tokenizer.word_index에 저장되어있음\n",
    "# 이렇게 만든 단어 집합에 있는 모든 단어를 사용하는 것이 아니라, 빈도수가 낮은 단어들은 훈련 데이터에서 제외하고 진행할 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "portable-neutral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 31981\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 23722\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8259\n",
      "단어 집합에서 희귀 단어의 비율: 74.17529157937525\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.3765843000014963\n"
     ]
    }
   ],
   "source": [
    "# 등장 빈도수가 7회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인\n",
    "# src_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장되어져 있는데, 이를 통해 통계적인 정보를 얻음\n",
    "\n",
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)   # encoder_input_train에 있는 총 단어수\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mighty-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등장 빈도가 6회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거\n",
    "# 위에서 이를 제외한 단어 집합의 크기가 대략 8000 안팍으로 계산되는데, 이와 비슷한 값으로 어림잡아 단어 집합의 크기를 8000으로 제한\n",
    "# 토크나이저를 정의할 때 num_words의 값을 정해주면, 단어 집합의 크기를 제한할 수 있음\n",
    "\n",
    "src_vocab = 8000\n",
    "src_tokenizer = Tokenizer(num_words = src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "existing-arcade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[134, 3897, 2415, 902, 886, 24, 718, 616, 3526, 106, 1065, 634, 622, 2645, 36, 3, 224, 64, 3801, 10, 4, 1332, 413, 1175], [1924, 151, 892, 12, 127, 712, 203, 517, 151, 156, 552, 28, 302, 122, 232, 4, 2, 491, 7, 984, 57, 212, 4, 2], [5, 251, 903, 123, 1203, 1171, 521, 627, 5, 16, 43, 453, 121, 5, 18, 256, 5, 227]]\n"
     ]
    }
   ],
   "source": [
    "# texts_to_sequences()는 생성된 단어 집합에 기반하여 입력으로 주어진 텍스트 데이터의 단어들을 모두 정수로 변환하는 정수 인코딩 수행\n",
    "# 현재 단어 집합의 크기를 8,000으로 제한했으니까 이제 8,000이 넘는 숫자들은 정수 인코딩 후에 데이터에 존재하지 않음\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "#잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])\n",
    "\n",
    "# 결과 : 텍스트를 정수로 바꿨으니까 정수로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "pointed-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary 데이터에 대해서도 동일한 작업 수행\n",
    "# 케라스의 토크나이저를 사용하여 decoder_input_train을 입력으로 전체 단어 집합과 각 단어에 대한 빈도수를 계산\n",
    "\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "\n",
    "# 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되고 이는 tar_tokenizer.word_index에 저장\n",
    "# tar_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "innovative-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 10565\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 8199\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2366\n",
      "단어 집합에서 희귀 단어의 비율: 77.60530052058684\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.9080652440203725\n"
     ]
    }
   ],
   "source": [
    "# 이를 통해서 통계적인 정보를 얻어서, 등장 빈도수가 6회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인\n",
    "\n",
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "light-policy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 391], [1, 149], [1, 1215, 631, 661], [1, 4, 13], [1, 1365, 1065, 158, 130, 11]]\n",
      "target\n",
      "decoder  [[391, 2], [149, 2], [1215, 631, 661, 2], [4, 13, 2], [1365, 1065, 158, 130, 11, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 위와 마찬가지로 등장빈도가 적은 희귀 단어들(대략 2000개) 제거 : 단어집합 크기 제한\n",
    "\n",
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words = tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "#잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-daily",
   "metadata": {},
   "source": [
    "##### 패딩 작업 전 점검 사항  \n",
    "전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 이제 빈(empty) 샘플이 되었을 가능성이 있어요. 이 현상은 길이가 상대적으로 길었던 원문(Text)의 경우에는 문제가 별로 없겠지만, 애초에 평균 길이가 4밖에 되지 않았던 요약문(Summary)의 경우에는 이 현상이 굉장히 두드러졌을 가능성이 높겠죠.  \n",
    "  \n",
    "요약문에서 길이가 0이 된 샘플들의 인덱스를 받아와볼게요. 여기서 주의할 점은 요약문인 decoder_input에는 sostoken 또는 decoder_target에는 eostoken이 추가된 상태이고, 이 두 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플수와 동일하게 매우 높으므로 단어 집합 제한에도 삭제 되지 않아요. 그래서 이제 길이가 0이 된 요약문의 실제길이는 1로 나올거에요. 길이 0이 된 decoder_input에는 sostoken, decoder_target에는 eostoken만 남아 있을테니까요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "binding-webcam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 1281\n",
      "삭제할 테스트 데이터의 개수 : 316\n",
      "훈련 데이터의 개수 : 51374\n",
      "훈련 레이블의 개수 : 51374\n",
      "테스트 데이터의 개수 : 12847\n",
      "테스트 레이블의 개수 : 12847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj42/anaconda3/envs/aiffel/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터와 테스트 데이터에서 요약문의 길이가 1인 경우의 인덱스를 각각 drop_train과 drop_test에 라는 변수에 저장\n",
    "# 이 샘플들은 모두 삭제\n",
    "\n",
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :',len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :',len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-flooring",
   "metadata": {},
   "source": [
    "#### 패딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "handled-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환 후, 서로 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰주는 패딩 작업 필요\n",
    "# 아까 정해두었던 최대 길이로 패딩함\n",
    "# 최대 길이보다 짧은 데이터들은 뒤의 공간에 숫자 0을 넣어 최대 길이로 길이를 맞춰주기\n",
    "\n",
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen = summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen = summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen = summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen = summary_max_len, padding='post')\n",
    "\n",
    "# 학습에 필요한 전처리 끝!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-moment",
   "metadata": {},
   "source": [
    "## 3. 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "speaking-handling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# 함수형 API를 이용하여 인코더를 설계\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-bunch",
   "metadata": {},
   "source": [
    "임베딩 벡터의 차원은 128로 정의하고, hidden state의 크기를 256으로 정의했어요. hidden state는 LSTM에서 얼만큼의 수용력(capacity)를 가질지를 정하는 파라미터에요. 이 파라미터는 LSTM의 용량의 크기나, LSTM에서의 뉴론의 갯수라고 이해하면 돼요. 다른 신경망과 마찬가지로, 무조건 용량을 많이 준다고 해서 성능이 반드시 올라가는 것은 아니에요.  \n",
    "  \n",
    "인코더의 LSTM은 총 3개의 층으로 구성해서 모델의 복잡도를 높였어요. hidden state의 크기를 늘리는 것이 LSTM 층 1개의 용량을 늘린다면, 3개의 층을 사용하는 것은 모델의 용량을 늘린다고 볼 수 있죠. 3개의 층을 지나서 인코더로부터 나온 출력 벡터는 디코더로 보내줘야겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "multiple-judge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "assured-portugal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2000)   514000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층 설계\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation = 'softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-midnight",
   "metadata": {},
   "source": [
    "디코더의 출력층에서는 Summary의 단어장인 tar_vocab의 수많은 선택지 중 하나의 단어를 선택하는 다중 클래스 분류 문제를 풀어야 해요. 그렇기 때문에 Dense의 인자로 tar_vocab을 주고, 활성화 함수로 소프트맥스 함수를 사용하고 있어요.  \n",
    "  \n",
    "지금까지 설계한 것은 인코더의 hidden state와 cell state를 디코더의 초기 state로 사용하는 가장 기본적인 seq2seq에요. 그런데 디코더의 출력층을 설계를 살짝 바꿔서 성능을 높일 수 있는 방법이 있어요! 바로 어텐션 메커니즘이에요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-identity",
   "metadata": {},
   "source": [
    "#### 어텐션 매커니즘  \n",
    "* 어텐션 메커니즘을 수행하는 어텐션 함수를 설계하는 것은 또 다른 새로운 신경망을 설계해야한다는 뜻  \n",
    "* 참고1 : https://wikidocs.net/73161  \n",
    "* 참고2 : https://www.youtube.com/watch?v=WsQLdu2JMgI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "artificial-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 깃허브에 공개되어져 있는 어텐션 함수를 다운로드\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer\n",
    "\n",
    "# 경로에 attention.py 파일이 생겼으니, 어텐션 메커니즘을 사용할 준비가 되었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "robust-booth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,276,432\n",
      "Trainable params: 4,276,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 설계한 디코더의 출력층을 다음과 같이 수정\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()\n",
    "\n",
    "# 위의 코드는 인코더의 hidden state들과 디코더의 hidden state들을 어텐션 함수의 입력으로 사용\n",
    "# 어텐션 함수가 리턴한 값을 예측 시에 디코더의 hidden state와 함께 활용하는 형태로 작동"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-serum",
   "metadata": {},
   "source": [
    "## 4. 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "formal-crowd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "201/201 [==============================] - 66s 327ms/step - loss: 2.7039 - val_loss: 2.3830\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 65s 324ms/step - loss: 2.3788 - val_loss: 2.2463\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 65s 325ms/step - loss: 2.2477 - val_loss: 2.1409\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 67s 332ms/step - loss: 2.1300 - val_loss: 2.0471\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 68s 339ms/step - loss: 2.0425 - val_loss: 1.9952\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 72s 360ms/step - loss: 1.9786 - val_loss: 1.9561\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 72s 359ms/step - loss: 1.9266 - val_loss: 1.9319\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 72s 359ms/step - loss: 1.8801 - val_loss: 1.9038\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 72s 360ms/step - loss: 1.8409 - val_loss: 1.8831\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 71s 353ms/step - loss: 1.8034 - val_loss: 1.8662\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 70s 347ms/step - loss: 1.7686 - val_loss: 1.8597\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 70s 348ms/step - loss: 1.7370 - val_loss: 1.8463\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 70s 348ms/step - loss: 1.7068 - val_loss: 1.8408\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 70s 348ms/step - loss: 1.6792 - val_loss: 1.8345\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 70s 348ms/step - loss: 1.6530 - val_loss: 1.8322\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 70s 348ms/step - loss: 1.6276 - val_loss: 1.8244\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 70s 348ms/step - loss: 1.6038 - val_loss: 1.8263\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 70s 348ms/step - loss: 1.5810 - val_loss: 1.8309\n",
      "Epoch 00018: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련(학습)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# EarlyStopping : 조기 종료, 특정 조건이 충족되면 모델의 훈련을 멈추는 역할\n",
    "# 여기서는 val_loss(검증 데이터의 손실)을 모니터링 하면서, 검증 데이터의 손실이 줄어들지 않고 증가하는 현상이 patiensce =2\n",
    "# 즉, 2회 관측되면 학습을 멈추도록 설정\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
    "\n",
    "history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 256, callbacks=[es], epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecological-border",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련 데이터의 손실과 검증 데이터의 손실이 줄어드는 과정을 시각화\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-athletics",
   "metadata": {},
   "source": [
    "## 5. 인퍼런스 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "coastal-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 단계에서 정수 인덱스 행렬로 존재하던 텍스트 데이터를 실제 데이터로 복원해야 하므로, 필요한 3개의 사전을 미리 준비\n",
    "\n",
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-jacksonville",
   "metadata": {},
   "source": [
    "##### seq2seq는 훈련할 때와 실제 동작할 때(인퍼런스 단계)의 방식이 다르므로 그에 맞게 모델 설계를 별개로 진행  \n",
    "훈련 단계에서는 디코더의 입력부에 정답이 되는 문장 전체를 한꺼번에 넣고 디코더의 출력과 한번에 비교할 수 있으므로, 인코더와 디코더를 엮은 통짜 모델 하나만 준비  \n",
    "  \n",
    "그러나 정답 문장이 없는 인퍼런스 단계에서는 만들어야 할 문장의 길이만큼 디코더가 반복 구조로 동작해야 하기 때문에 부득이하게 인퍼런스를 위한 모델 설계를 별도로 해주어야 함  \n",
    "  \n",
    "이 때는 인코더 모델과 디코더 모델을 분리해서 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "advanced-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "balanced-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "assured-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인퍼런스 단계에서 단어 시퀀스를 완성하는 함수 만들기\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-mitchell",
   "metadata": {},
   "source": [
    "## 6. 모델 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "hydraulic-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 정수 시퀀스를 텍스트 시퀀스로 변환하는 함수를 만들어 두기\n",
    "# 1. Text의 정수 시퀀스에서는 패딩을 위해 사용되는 숫자 0을 제외시키고\n",
    "# 2. Summary의 정수 시퀀스에서는 숫자 0, 시작 토큰의 인덱스, 종료 토큰의 인덱스를 출력에서 제외\n",
    "\n",
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "oriental-bible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : lowrey probably best pork rinds available usa always fresh proper time micro wave \n",
      "실제 요약 : best \n",
      "예측 요약 :  best peanuts ever\n",
      "\n",
      "\n",
      "원문 : first tried tasty almonds amazed great flavor kick wasabi sweet taste roasted almonds first one eye opener time even many times bold fun like sushi enjoy taste wasabi \n",
      "실제 요약 : different and tasty \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : sent lb bag son iraq said best gummi bears ever eaten bought world haribo original manufacturer gummi bears remember order gold bears highly recommend thanks amazon \n",
      "실제 요약 : best ever \n",
      "예측 요약 :  gummi bears\n",
      "\n",
      "\n",
      "원문 : tried five years ago greek cafe sweet pungent flavor anise looked years without success days ago ordered arrived day estimated arrival could happier price wonderful \n",
      "실제 요약 : wonderful flavor \n",
      "예측 요약 :  best maple syrup ever\n",
      "\n",
      "\n",
      "원문 : wish breakfast items tasted good almost like eating cookie breakfast tip heat makes taste even better \n",
      "실제 요약 : perfect tasty breakfast \n",
      "예측 요약 :  good for gluten free\n",
      "\n",
      "\n",
      "원문 : wife brought five different flavored excellent used taco flavor last night best tacos made yet \n",
      "실제 요약 : gourmet \n",
      "예측 요약 :  great flavor\n",
      "\n",
      "\n",
      "원문 : never huge coffee fan however mother purchased little machine talked trying latte coffee shop better one like products little machine super easy use really good coffee latte cappuccino etc less minute would recommend dolce gusto anyone good price getting one \n",
      "실제 요약 : great machine \n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : cadbury used whole hazelnut chocolate changed hazelnut crumbles ruined slightly creamy cadbury chocolate bit creamy taste complaining excellent nut chocolate ratio \n",
      "실제 요약 : whole \n",
      "예측 요약 :  chocolate\n",
      "\n",
      "\n",
      "원문 : excited saw product stated sweetened honey see anywhere also contained sucralose dislike much actually received product also found time purchase cannot returned amazon please future list ingredients products \n",
      "실제 요약 : disappointed \n",
      "예측 요약 :  not as advertised\n",
      "\n",
      "\n",
      "원문 : would buy product nothing near korean brand pho could find amazon \n",
      "실제 요약 : not too good \n",
      "예측 요약 :  not\n",
      "\n",
      "\n",
      "원문 : organic antioxidant omega rich air dried venison loaded recommended essential vitamin nutrients definitely one best dog food products market feel good giving dog human grade quality food processed supports raw food diet nature intended \n",
      "실제 요약 : my yorkie loves \n",
      "예측 요약 :  great food for the price\n",
      "\n",
      "\n",
      "원문 : fine coffee indeed roast dark light believe beans mexico clean effect get jittery like coffee beans highly recommended \n",
      "실제 요약 : love cafe \n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : arrived well packed damage came april lush green slightly different pot site like one got better week nothing shown signs would definatly buy \n",
      "실제 요약 : great little tree \n",
      "예측 요약 :  good tree\n",
      "\n",
      "\n",
      "원문 : yummy black sesame cheddar vegetable flavors excellent low fat gluten free hard find local grocery stores around least \n",
      "실제 요약 : best rice crackers ever \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : coffee light side mild medium roast right wake morning continue buy blend subscribe save rate price count excellent ways seems great purchase \n",
      "실제 요약 : san francisco breakfast blend coffee for keurig \n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : absolutely delicious honey know health benefits spoonful day three weeks notice anything health wise taste natural amazing \n",
      "실제 요약 : yum \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : great tasting product much better cocoas tasted type single serve job well dont grove square \n",
      "실제 요약 : great product \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : product arrived dried size package clearly past due date keep squirrels away buy recommended \n",
      "실제 요약 : dried out and past \n",
      "예측 요약 :  not as advertised\n",
      "\n",
      "\n",
      "원문 : hit two cavalier king charles spaniels nutritious snacks one newman company newman fan ever since got hooked tasty pasta sauce dogs love sure recommended \n",
      "실제 요약 : my doggies love em \n",
      "예측 요약 :  great for the\n",
      "\n",
      "\n",
      "원문 : asin erin baker breakfast cookies oatmeal raisin ounce individually wrapped cookies love cookies breakfast snack keep feeling full hours give lots energy feel good since healthy yummy \n",
      "실제 요약 : love these cookies \n",
      "예측 요약 :  great for breakfast\n",
      "\n",
      "\n",
      "원문 : bold jalapeno wasabi soy almonds taste great make good snack party treat \n",
      "실제 요약 : blue diamond almonds \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : came upon coffee chance local store loved flavor later internet search found amazon plus available subscription talk convenience \n",
      "실제 요약 : great coffee \n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : wife love tastes great healthier sodas would definitely purchase maybe even try flavors \n",
      "실제 요약 : tastes really good \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : pops nice never get taste like movie theater popcorn even come close gave star popping taste \n",
      "실제 요약 : do not like \n",
      "예측 요약 :  great popcorn\n",
      "\n",
      "\n",
      "원문 : excellent tea fruit flavor right amount water bottles definitely recommended product nice convenient pouch easy store \n",
      "실제 요약 : excellent tea flavor \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : product worth money makes asian food easy make kitchen smells days cooking get picky children \n",
      "실제 요약 : so good \n",
      "예측 요약 :  great for\n",
      "\n",
      "\n",
      "원문 : subscribed saved kind jerky past subscription canceled time since jerky expensive piece recently subscription cancel second shipment nasty got two boxes know old moldy disgusting disappointed product recommend people avoid company steps improves quality may better deal buy quantity cannot even eat \n",
      "실제 요약 : very \n",
      "예측 요약 :  do not buy\n",
      "\n",
      "\n",
      "원문 : super disappointed orbit discontinued favorite flavor gum many flavors like came thrilled appeared gone amazon true word let know gum still ordered two cartons order based continued availability thanks amazon \n",
      "실제 요약 : they found my favorite gum \n",
      "예측 요약 :  gum\n",
      "\n",
      "\n",
      "원문 : love product concentrated flavors pack punch however disappointed flavor chai really taste like chai tea still search perfect chai tea concentrate flavors great favorite gingerbread \n",
      "실제 요약 : chai is so so \n",
      "예측 요약 :  my favorite tea\n",
      "\n",
      "\n",
      "원문 : love flavor cant find shelves anymore gum tasted fresh actually way better used buy stores \n",
      "실제 요약 : great gum \n",
      "예측 요약 :  best gum ever\n",
      "\n",
      "\n",
      "원문 : fast shipping already devoured tried cheese nacho wisconsin ranch styles ranch time favorite going increase frequency delivery \n",
      "실제 요약 : love these \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : outrageous sold added meat parts make whole product parts highly likely cause sickness people etc product us everywhere else human hazard chemical practice every place really \n",
      "실제 요약 : this is disgusting and \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : use lot different flavoured herbal teas usually look flavoured tea good one also adds little combining enjoy \n",
      "실제 요약 : an excellent tea \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : decent clean simple cup morning decaf find pre pod helps extract little extra flavor \n",
      "실제 요약 : decent \n",
      "예측 요약 :  great for the price\n",
      "\n",
      "\n",
      "원문 : diagnosed celiac disease dec tried lot different rice noodles brand far best tried gummy sticky like others tinkyada brown rice spaghetti rice bran ounce packages \n",
      "실제 요약 : gluten free \n",
      "예측 요약 :  best gluten free pasta\n",
      "\n",
      "\n",
      "원문 : brothers cats food whole lives love healthy times hard find stores prefer order online \n",
      "실제 요약 : cat meow \n",
      "예측 요약 :  cat food\n",
      "\n",
      "\n",
      "원문 : warning dog owners edible nylabones extremely dangerous dogs big appetite dog swallowed whole within seconds giving choked fully could bone breathe nose bring back life almost lost bone dog chewer eater please buy edible nylabones \n",
      "실제 요약 : my dog on the edible bone \n",
      "예측 요약 :  dog chews\n",
      "\n",
      "\n",
      "원문 : bought pen write letter cookies tip fine expected maybe personal preference try cake pops made candy melts pen stopped working worked since try another brand next time \n",
      "실제 요약 : not what was expected \n",
      "예측 요약 :  not so good\n",
      "\n",
      "\n",
      "원문 : far favorite thing bike going hour ride try eat one even get started one around halfway point taste great extremely important \n",
      "실제 요약 : love these \n",
      "예측 요약 :  love it\n",
      "\n",
      "\n",
      "원문 : small dogs one really small dog get one greenie day love greenies love greenies help keep mouths fresh clean adorable shaped like mini toothbrush \n",
      "실제 요약 : my love greenies \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : luck find product local grocery store one day tried sale proceeded go back buy remaining stock left store describe refreshing water let say bottle stream arctic would kind close unfortunately reason sale grocery discontinued carrying product hard find ever since good see amazon least knows good product taste one \n",
      "실제 요약 : hard to find in stores worth the \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : love salsa best salsa ever eaten order salsa crave cannot find stores would recommend salsa day \n",
      "실제 요약 : love salsa \n",
      "예측 요약 :  best olive oil ever\n",
      "\n",
      "\n",
      "원문 : soap turn mushy excellent skin natural really smart bargain \n",
      "실제 요약 : lasting \n",
      "예측 요약 :  best dressing ever\n",
      "\n",
      "\n",
      "원문 : disappointed display show china trying avoid made china \n",
      "실제 요약 : disappointed \n",
      "예측 요약 :  made in china\n",
      "\n",
      "\n",
      "원문 : looking special treat special dog believe produce fill need small mouth dog \n",
      "실제 요약 : service dogs treats \n",
      "예측 요약 :  my dogs love these\n",
      "\n",
      "\n",
      "원문 : perfect entertain coffee also conversation win win \n",
      "실제 요약 : for the \n",
      "예측 요약 :  great for the price\n",
      "\n",
      "\n",
      "원문 : pure smooth sugar sweet best price since sams club stop selling syrup \n",
      "실제 요약 : real syrup \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : great way sweeten anything plus toffee flavor makes even plain coffee taste like scrumptious desert yummy \n",
      "실제 요약 : tasty stevia \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : admit bought gluten free foods web site completely missed calorie description expecting tasty gluten free ranch dressing least recognizable ranch cannot believe two reviews stuff flavor well true flavor something like might get took dirty dish dipped paint salad vile nasty stuff waste money one \n",
      "실제 요약 : what the \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : love way spices smaller containers without spices getting old smaller containers foil packets keep spices fresh day got great idea \n",
      "실제 요약 : spices \n",
      "예측 요약 :  great deal\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 약 50개의 샘플에 대해서 실제 요약과 예측된 요약을 비교\n",
    "\n",
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-concert",
   "metadata": {},
   "source": [
    "## 7. 추출적 요약 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-double",
   "metadata": {},
   "source": [
    "앞서 seq2seq를 통해서 추상적 요약을 진행해봤어요. 그런데 텍스트 요약에는 추상적 요약 외에도 이미 본문에 존재하는 단어구, 문장을 뽑아서 요약으로 삼는 추출적 요약 방법도 있었죠.  \n",
    "  \n",
    "패키지 Summa에서는 추출적 요약을 위한 모듈인 summarize를 제공하고 있어 아주 간단하게 실습을 해볼 수 있어요. 영화 매트릭스 시놉시스를 요약해보면서 summarize 사용법을 익혀볼까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-traffic",
   "metadata": {},
   "source": [
    "#### 패키지 설치  \n",
    "* `pip install summa`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-cliff",
   "metadata": {},
   "source": [
    "#### 데이터 다운로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ready-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "metropolitan-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매트릭스 시놉시스를 다운로드\n",
    "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text\n",
    "\n",
    "# text에 매트릭스 시놉시스가 문자열로 저장되었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "theoretical-january",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The screen is filled with green, cascading code which gives way to the title, The Matrix.\r\n",
      "\r\n",
      "A phone rings and text appears on the screen: \"Call trans opt: received. 2-19-98 13:24:18 REC: Log>\" As a conversation takes place between Trinity (Carrie-Anne Moss) and Cypher (Joe Pantoliano), two free humans, a table of random green numbers are being scanned and individual numbers selected, creating a series of digits not unlike an ordinary phone number, as if a code is being deciphered or a call is being traced.\r\n",
      "\r\n",
      "Trinity discusses some unknown person. Cypher taunts Trinity, suggesting she enjoys watching him. Trinity counters that \"Morpheus (Laurence Fishburne) says he may be 'the One',\" just as the sound of a number being selected alerts Trinity that someone may be tracing their call. She ends the call.\r\n",
      "\r\n",
      "Armed policemen move down a darkened, decrepit hallway in the Heart O' the City Hotel, their flashlight beam bouncing just ahead of them. They come to room 303, kick down the door and find a woman dressed in black, facing away from them. It's Trinity. She brings her hands up from the laptop she's working on at their command.\r\n",
      "\r\n",
      "Outside the hotel a car drives up and three agents appear in neatly pressed black suits. They are Agent Smith (Hugo Weaving), Agent Brown (Paul Goddard), and Agent Jones (Robert Taylor). Agent Smith and the presiding police lieutenant argue. Agent Smith admonishes the policeman that they were given specific orders to contact the agents first, for their\n"
     ]
    }
   ],
   "source": [
    "# 출력 결과가 아주 길기 때문에 일부만 출력해보고, 잘 저장이 되었는지 확인해보기\n",
    "\n",
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-stick",
   "metadata": {},
   "source": [
    "#### summarize 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-plaza",
   "metadata": {},
   "source": [
    "* text (str) : 요약할 테스트  \n",
    "* ratio (float, optional) – 요약문에서 원본에서 선택되는 문장 비율. 0~1 사이값  \n",
    "* words (int or None, optional) – 출력에 포함할 단어 수  \n",
    "* 만약, ratio와 함께 두 파라미터가 모두 제공되는 경우 ratio는 무시  \n",
    "* split (bool, optional) – True면 문장 list / False는 조인(join)된 문자열을 반환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "precious-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "# Summa의 summarize는 문장 토큰화를 별도로 하지 않더라도 내부적으로 문장 토큰화를 수행\n",
    "# 그렇기 때문에 문장 구분이 되어있지 않은 원문을 바로 입력으로 넣을 수 있음\n",
    "# 비율을 적게 주어 요약문으로 선택되는 문장의 개수를 줄여보기\n",
    "# 원문의 0.005%만을 출력하도록 설하\n",
    "\n",
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bacterial-tunnel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "['Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.', 'Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.']\n"
     ]
    }
   ],
   "source": [
    "# 리스트로 출력 결과를 받고 싶으면 split 인자의 값을 True로 하기\n",
    "\n",
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "victorian-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Trinity takes Neo to Morpheus.\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "# 단어의 수로 요약문의 크기를 조절 가능\n",
    "# 단어를 50개만 선택하도록 설정\n",
    "\n",
    "print('Summary:')\n",
    "print(summarize(text, words=50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
